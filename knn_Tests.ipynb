{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import utils.ImagesProcessor as ip\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import itertools\n",
    "\n",
    "import models.CAE as cae\n",
    "\n",
    "slicesSize = np.array([28, 28, 3])\n",
    "overlap = 10\n",
    "ratioTrainTest = 0.8\n",
    "inputShape = np.array([300, 300, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImages(directory_name):\n",
    "    directory = os.fsencode(directory_name)\n",
    "    imgs = []\n",
    "    filenames = []\n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        if filename.lower().endswith(\".jpg\"): \n",
    "            img = IP.readImage(directory_name + \"/\" + filename)\n",
    "            img = IP.resizeImage(img, inputShape[:-1])\n",
    "            img = IP.extractChromaticity(img)\n",
    "            img_slices = IP.sliceImage(img,(slicesSize[0], slicesSize[1]),overlap)\n",
    "            imgs.append(img_slices)\n",
    "            filenames.append(filename)\n",
    "    return np.array(imgs), filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:313: RuntimeWarning: divide by zero encountered in log\n",
      "  log_a = np.log(a)\n"
     ]
    }
   ],
   "source": [
    "TRAINING_PATH = \"dataset_train\"\n",
    "NORMAL_PATH = \"dataset_normal\"\n",
    "ANOMALY_PATH = \"dataset_anomaly\"\n",
    "IP = ip.ImagesProcessor()\n",
    "\n",
    "# Load the images\n",
    "X_train, trainFilenames = loadImages(TRAINING_PATH)\n",
    "X_normal, normalFilenames= loadImages(NORMAL_PATH)\n",
    "X_anomaly, anomalyFilenames = loadImages(ANOMALY_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 289, 28, 28, 3)\n",
      "(12, 289, 28, 28, 3)\n",
      "(14, 289, 28, 28, 3)\n"
     ]
    }
   ],
   "source": [
    "# Normalize them\n",
    "X_train = X_train.astype('float32')/255.0\n",
    "X_normal = X_normal.astype('float32')/255.0\n",
    "X_anomaly = X_anomaly.astype('float32')/255.0\n",
    "\n",
    "print(X_train.shape)\n",
    "#print(X_train)\n",
    "print(X_normal.shape)\n",
    "#print(X_normal)\n",
    "print(X_anomaly.shape)\n",
    "#print(X_anomaly)\n",
    "#print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidienne_distance(data1, data2):    \n",
    "    return np.sum(np.power(np.array(data1) - np.array(data2), 2))\n",
    "\n",
    "\n",
    "def get_neighbors(training_set, \n",
    "          data_point, \n",
    "          k, \n",
    "          distance=euclidienne_distance):\n",
    "    #print(data_point.shape)\n",
    "    #print(training_set.shape)\n",
    "    distances = []\n",
    "    for indexTrain in range(len(training_set)):\n",
    "        dist = distance(data_point, training_set[indexTrain])\n",
    "        distances.append(dist)\n",
    "    distances.sort()\n",
    "    neighbors = distances[:k]\n",
    "    mean_distance = np.sum(neighbors)\n",
    "    return mean_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidienne_distance([1,2,3], [1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore  parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testKNN():\n",
    "    x_temp = np.random.permutation(X_train.reshape(-1, slicesSize))\n",
    "    x_train = x_temp[:int(ratioTrainTest*len(X_train))]\n",
    "    x_test = x_temp[int(ratioTrainTest*len(X_train)):]\n",
    "    autoencoder = cae.CAE(inputShape,nbNeuronsLayers=[16, 8, 8], nbConvFilters=(3,3), poolScale=(2, 2))\n",
    "    autoencoder.createModel()\n",
    "    autoencoder.train(x_train, x_test, epochs=300, batch_size=64)\n",
    "    \n",
    "    pred_ref = autoencoder.extractFeatures(X_train.reshape(-1, slicesSize))\n",
    "    print(\"ref:\", pred_ref)\n",
    "    #print(trainFilenames)\n",
    "    pred_nor = autoencoder.extractFeatures(X_normal)\n",
    "    print(\"normal:\", pred_nor)\n",
    "    #print(normalFilenames)\n",
    "    pred_ano = autoencoder.extractFeatures(X_anomaly)\n",
    "    print(\"anomalies:\", pred_ano)\n",
    "    #print(anomalyFilenames)\n",
    "    return accuracy_score(np.append(pred_nor, pred_ano), [1]*len(pred_nor) + [-1]*len(pred_ano))\n",
    "#print(testCAE())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnbNeuronsLayer1 = [128, 64, 32, 16, 8]\\nnbNeuronsLayer2 = [128, 64, 32, 16, 8]\\n\\nfig = plt.figure(num=None, figsize=(15, 10), dpi=80, facecolor='w', edgecolor='k')\\nax = fig.gca(projection='3d')\\n\\n# Make data.\\nnbNeuronsLayer1, nbNeuronsLayer2 = np.meshgrid(nbNeuronsLayer1, nbNeuronsLayer2)\\n\\naccurencies = np.array([testCAE(x,y) for x,y in zip(np.ravel(nbNeuronsLayer1), np.ravel(nbNeuronsLayer2))])\\naccurencies = accurencies.reshape(nbNeuronsLayer2.shape)\\n\\ngraph = ax.plot_surface(nbNeuronsLayer1, nbNeuronsLayer2, accurencies, cmap=cm.coolwarm,\\n                       linewidth=0, antialiased=False)\\n\\nax.set_xlabel('nbNeuronsLayer1')\\nax.set_ylabel('nbNeuronsLayer2 Parameter')\\nax.set_zlabel('Accuracy')\\n\\n# Customize the z axis.\\nax.set_zlim(0, 1.00)\\nax.zaxis.set_major_locator(LinearLocator(10))\\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\\n\\n# Add a color bar which maps values to colors.\\nfig.colorbar(graph, shrink=0.5, aspect=5)\\n\\nplt.show()\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "nbNeuronsLayer1 = [128, 64, 32, 16, 8]\n",
    "nbNeuronsLayer2 = [128, 64, 32, 16, 8]\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(15, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "# Make data.\n",
    "nbNeuronsLayer1, nbNeuronsLayer2 = np.meshgrid(nbNeuronsLayer1, nbNeuronsLayer2)\n",
    "\n",
    "accurencies = np.array([testCAE(x,y) for x,y in zip(np.ravel(nbNeuronsLayer1), np.ravel(nbNeuronsLayer2))])\n",
    "accurencies = accurencies.reshape(nbNeuronsLayer2.shape)\n",
    "\n",
    "graph = ax.plot_surface(nbNeuronsLayer1, nbNeuronsLayer2, accurencies, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "\n",
    "ax.set_xlabel('nbNeuronsLayer1')\n",
    "ax.set_ylabel('nbNeuronsLayer2 Parameter')\n",
    "ax.set_zlabel('Accuracy')\n",
    "\n",
    "# Customize the z axis.\n",
    "ax.set_zlim(0, 1.00)\n",
    "ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "\n",
    "# Add a color bar which maps values to colors.\n",
    "fig.colorbar(graph, shrink=0.5, aspect=5)\n",
    "\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "Train on 5 samples, validate on 2018 samples\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.0349 - val_loss: 0.0340\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 1s 101ms/step - loss: 0.0340 - val_loss: 0.0333\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.0334 - val_loss: 0.0329\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.0329 - val_loss: 0.0325\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.0326 - val_loss: 0.0323\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 1s 102ms/step - loss: 0.0323 - val_loss: 0.0320\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.0320 - val_loss: 0.0316\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 1s 100ms/step - loss: 0.0317 - val_loss: 0.0313\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 1s 100ms/step - loss: 0.0313 - val_loss: 0.0308\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.0308 - val_loss: 0.0303\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 1s 101ms/step - loss: 0.0303 - val_loss: 0.0297\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.0297 - val_loss: 0.0290\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.0289 - val_loss: 0.0281\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 1s 100ms/step - loss: 0.0280 - val_loss: 0.0270\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.0269 - val_loss: 0.0256\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.0255 - val_loss: 0.0240\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.0239 - val_loss: 0.0221\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.0219 - val_loss: 0.0198\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.0195 - val_loss: 0.0172\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.0168 - val_loss: 0.0142\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.0138 - val_loss: 0.0112\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.0107 - val_loss: 0.0083\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 1s 105ms/step - loss: 0.0076 - val_loss: 0.0061\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.0053 - val_loss: 0.0055\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.0045 - val_loss: 0.0068\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.0056 - val_loss: 0.0088\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.0076 - val_loss: 0.0098\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 1s 100ms/step - loss: 0.0085 - val_loss: 0.0094\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.0082 - val_loss: 0.0082\n",
      "errors_ref:  [0.00517466 0.00519793 0.00520684 ... 0.01582159 0.01588236 0.0164172 ]\n",
      "delta_error:  0.009507972747087479\n"
     ]
    }
   ],
   "source": [
    "x_temp = X_train.reshape(-1, slicesSize[0], slicesSize[1], slicesSize[2])\n",
    "x_temp = np.random.permutation(x_temp)\n",
    "x_train = x_temp[:int(ratioTrainTest*len(X_train))]\n",
    "x_test = x_temp[int(ratioTrainTest*len(X_train)):]\n",
    "autoencoder = cae.CAE(slicesSize,nbNeuronsLayers=[16, 8, 8], nbConvFilters=(3,3), poolScale=(2, 2))\n",
    "autoencoder.createModel()\n",
    "autoencoder.train(x_train, x_test, epochs=300, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 289, 4, 4, 8)\n",
      "(12, 289, 4, 4, 8)\n",
      "(14, 289, 4, 4, 8)\n",
      "deltaError= 0.011968583799898624\n",
      "NA_0209.jpg  - classe :  1\n",
      "NA_0247.jpg  - classe :  -1\n",
      "NA_0244.jpg  - classe :  1\n",
      "NA_0268.jpg  - classe :  1\n",
      "NA_0240.jpg  - classe :  -1\n",
      "NA_0267.jpg  - classe :  1\n",
      "NA_0266.jpg  - classe :  1\n",
      "NA_0261.jpg  - classe :  -1\n",
      "NA_0212.jpg  - classe :  1\n",
      "NA_0216.jpg  - classe :  1\n",
      "NA_0203.jpg  - classe :  1\n",
      "NA_0200.jpg  - classe :  1\n",
      "A_0254.jpg  - classe :  -1\n",
      "A_0255.jpg  - classe :  -1\n",
      "A_0241.jpg  - classe :  -1\n",
      "A_0257.jpg  - classe :  -1\n",
      "A_0242.jpg  - classe :  -1\n",
      "A_0256.jpg  - classe :  -1\n",
      "A_0253.jpg  - classe :  -1\n",
      "A_0251.jpg  - classe :  -1\n",
      "A_0236.jpg  - classe :  -1\n",
      "A_0205.jpg  - classe :  1\n",
      "A_0249.jpg  - classe :  -1\n",
      "A_0248.jpg  - classe :  -1\n",
      "A_0258.jpg  - classe :  -1\n",
      "A_0259.jpg  - classe :  -1\n"
     ]
    }
   ],
   "source": [
    "X_train_features = []\n",
    "for img in X_train:\n",
    "    X_train_features.append(autoencoder.extractFeatures(img))\n",
    "X_train_features = np.array(X_train_features)\n",
    "print(X_train_features.shape)\n",
    "\n",
    "X_normal_features = []\n",
    "for img in X_normal:\n",
    "    X_normal_features.append(autoencoder.extractFeatures(img))\n",
    "X_normal_features = np.array(X_normal_features)\n",
    "print(X_normal_features.shape)\n",
    "\n",
    "X_anomaly_features = []\n",
    "for img in X_anomaly:\n",
    "    X_anomaly_features.append(autoencoder.extractFeatures(img))\n",
    "X_anomaly_features = np.array(X_anomaly_features)\n",
    "print(X_anomaly_features.shape)\n",
    "\n",
    "train_distances = []\n",
    "for img_index in range(X_train_features.shape[0]):\n",
    "    slices_distances = []\n",
    "    for slice_img in X_train_features[img_index]:\n",
    "        slices_distances.append(get_neighbors(X_train_features.reshape(-1, X_train_features.shape[2], X_train_features.shape[3], X_train_features.shape[4]), slice_img, 5))\n",
    "    train_distances.append(np.mean(slices_distances))\n",
    "    #print(trainFilenames[img_index], \" - mean : \", np.mean(slices_distances))\n",
    "\n",
    "deltaError = np.percentile(train_distances, 75)\n",
    "print(\"deltaError=\", deltaError)\n",
    "\n",
    "test_distances = []\n",
    "for img_index in range(X_normal_features.shape[0]):\n",
    "    slices_distances = []\n",
    "    for slice_img in X_normal_features[img_index]:\n",
    "        slices_distances.append(get_neighbors(X_train_features.reshape(-1, X_train_features.shape[2], X_train_features.shape[3], X_train_features.shape[4]), slice_img, 4))\n",
    "    mean = np.mean(slices_distances)\n",
    "    if mean < deltaError:\n",
    "        classe = 1\n",
    "    else:\n",
    "        classe = -1\n",
    "    print(normalFilenames[img_index], \" - classe : \", classe)\n",
    "    test_distances.append(classe)\n",
    "    \n",
    "for img_index in range(X_anomaly_features.shape[0]):\n",
    "    slices_distances = []\n",
    "    for slice_img in X_anomaly_features[img_index]:\n",
    "        slices_distances.append(get_neighbors(X_train_features.reshape(-1, X_train_features.shape[2], X_train_features.shape[3], X_train_features.shape[4]), slice_img, 4))\n",
    "    mean = np.mean(slices_distances)\n",
    "    if mean < deltaError:\n",
    "        classe = 1\n",
    "    else:\n",
    "        classe = -1\n",
    "    test_distances.append(classe)\n",
    "    print(anomalyFilenames[img_index], \" - classe : \", classe)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c40a86ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf_mat = confusion_matrix(test_distances, [1]*len(X_normal_features) + [-1]*len(X_anomaly_features))\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):     \n",
    "    labels_names_ref = [\"Anomalie\", \"Normal\"]\n",
    "        \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels_names_ref))\n",
    "    plt.xticks(tick_marks, labels_names_ref, rotation=45)\n",
    "    plt.yticks(tick_marks, labels_names_ref)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Predicted label')\n",
    "    plt.xlabel('True label')\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "plot_confusion_matrix(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8461538461538461\n",
      "Precision:  0.8392857142857143\n",
      "Recall:  0.85625\n",
      "F1-Score:  0.84768299104792\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(test_distances, [1]*len(X_normal_features) + [-1]*len(X_anomaly_features)))\n",
    "precision = (conf_mat[0][0]/(conf_mat[0][0] + conf_mat[1][0]) + conf_mat[1][1]/(conf_mat[1][1] + conf_mat[0][1]))/2\n",
    "print(\"Precision: \", precision)\n",
    "recall = (conf_mat[0][0]/(conf_mat[0][0] + conf_mat[0][1]) + conf_mat[1][1]/(conf_mat[1][1] + conf_mat[1][0]))/2\n",
    "print(\"Recall: \", recall)\n",
    "f1score = 2*(precision*recall)/(precision+recall)\n",
    "print(\"F1-Score: \", f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
